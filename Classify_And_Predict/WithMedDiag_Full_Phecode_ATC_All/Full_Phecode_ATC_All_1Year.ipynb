{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9947eea4-1f70-40a5-bdbd-eec947e8abdf",
   "metadata": {},
   "source": [
    "#### Task: Classification and Predicition of readmission with medication and diagnosis columns, based on Phecodes and ATC codes \n",
    "**`TNE_BO_365` values (0 and 1 are used to classify readmission, where 1 means readmission). Further for prediction only the readmitted patient data is taken and a linear regression model is trained on it to predict the number of days for readmission.**\n",
    "- Dataset Used: `/home/kabank/workbench/.conda/analysis/kabank-data/new-data/Full_Phecode_ATC.csv`\n",
    "- Independent variable: `'age', 'remaining_time_countdown','var_no_dates_permonth', 'gender','closingcode', 'aftercode', 'Length_of_Episode', \n",
    "                                                   'Count_visit','Therapy_ratio', 'Examination_ratio', 'Advisory_ratio',\n",
    "                                                   'TreatmentPlanning_ratio', 'Outpatient_ratio','Inpatient_daynight_ratio', 'Care_intensity', 'num_diagnoses', 'num_medications'`\n",
    "- Dependent Variable: For classification its `TNE_BO_365` and for prediction its `TNE_NO_365`\n",
    "- Other Important Info: Dataset has NaN values. NaN has been replaced with 0\n",
    "\n",
    "<br>\n",
    "Description of work done:\n",
    "\n",
    "1. In this scripts the first class `SplitMedicationDiagnosisInUniquePieces` splits all the diagnosia and medication column into unique pieces.\n",
    "2. Then it saves the new data frame with unique diagnosis and medication as new `CSV` file named as `Split_Full_Phecode_ATC`\n",
    "3. Then the second class `ClassifyReadmissionWithMedicationDiagnosis` takes the new `Split_Full_Phecode_ATC` dataset and perform classiciation and prediction\n",
    "4. To run the script replace if you are not using `dotenv` replace`os.getenv(\"FILE_Full_Phecode_ATC_PATH\")` with the path of your file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d678b9-83cc-485e-8572-6ed9ed9d9558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import os\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "\n",
    "\n",
    "######### Split Medication and Diagnosis In Unique Pieces ############\n",
    "#                                                                    #\n",
    "#                                                                    #\n",
    "#                                                                    #\n",
    "######################################################################\n",
    "\n",
    "class SplitMedicationDiagnosisInUniquePieces:\n",
    "    def __init__(self, file1):\n",
    "        self.file1 = file1\n",
    "        self.merged_df = None\n",
    "        self.selected_column_merged_df = None\n",
    "        #self.with_diag_medic_selected_column_merged_df = None\n",
    "    \n",
    "    def load_data_to_split(self):\n",
    "        Full_Phecode_ATC = pd.read_csv(self.file1)\n",
    "        self.merged_df = Full_Phecode_ATC \n",
    "        self.merged_df = self.merged_df[['episode_id', 'num_diagnoses', 'num_medications', 'pasient', 'age',\n",
    "                                       'remaining_time_countdown', 'var_no_dates_permonth', 'gender',\n",
    "                                       'episode_order', 'islast', 'closingcode', 'aftercode',\n",
    "                                       'episode_start_date', 'episode_end_date', 'tillnextepisode',\n",
    "                                       'Length_of_Episode', 'Cat_LOE', 'TNE_BO_180', 'TNE_NO_180',\n",
    "                                       'TNE_BO_365', 'TNE_NO_365', 'TNE_BO_730', 'TNE_NO_730', 'TNE_BO_1095',\n",
    "                                       'TNE_NO_1095', 'Cat_LOE_desc', 'Count_visit', 'Cat_CV', 'Therapy_ratio',\n",
    "                                       'Examination_ratio', 'Advisory_ratio', 'TreatmentPlanning_ratio',\n",
    "                                       'Outpatient_ratio', 'Inpatient_ratio', 'Inpatient_day_ratio',\n",
    "                                       'Inpatient_daynight_ratio', 'Care_intensity', 'age_group',\n",
    "                                       'closingcode_0', 'closingcode_1', 'closingcode_2', 'closingcode_3',\n",
    "                                       'closingcode_4', 'closingcode_5', 'closingcode_6', 'closingcode_9',\n",
    "                                       'aftercode_1', 'aftercode_2', 'aftercode_3', 'aftercode_4',\n",
    "                                       'aftercode_5', 'gender_0', 'F', 'M', 'MiddleChildhood', 'Preschooler',\n",
    "                                       'Teenager', 'actual_diag_Phe', 'actual_med_Full_ATC']]\n",
    "        \n",
    "        self.merged_df = self.merged_df.copy(deep=True)\n",
    "        return self.merged_df\n",
    "\n",
    "        \n",
    "    def split_diagnosis_medication_in_unique_pieces(self):\n",
    "        newmerged_df = SplitMedicationDiagnosisInUniquePieces_Obj.load_data_to_split()\n",
    "        columns = ['actual_diag_Phe', 'actual_med_Full_ATC']\n",
    "        for col in columns:\n",
    "            newmerged_df[col] = newmerged_df[col].apply(lambda d:[] if pd.isnull(d) else d)\n",
    "            newmerged_df[col] = newmerged_df[col].str.replace(\"[\", \"\")\n",
    "            newmerged_df[col] = newmerged_df[col].str.replace(\"]\", \"\")\n",
    "            newmerged_df[col] = newmerged_df[col].str.replace(\"'\", \"\")\n",
    "            newmerged_df[col] = newmerged_df[col].str.replace(\" \", \"\")\n",
    "            newmerged_df[col] = newmerged_df[col].str.split(',')\n",
    "            newmerged_df = newmerged_df.explode(col)\n",
    "            \n",
    "            # Show columns names\n",
    "            temp_store = newmerged_df[col].value_counts()\n",
    "            names_only = temp_store.index.tolist()\n",
    "            \n",
    "            original_columns = newmerged_df.columns.tolist()\n",
    "            dummies = newmerged_df[col].str.get_dummies(sep=',')\n",
    "            newmerged_df = pd.concat([newmerged_df, dummies], axis=1)\n",
    "            newmerged_df = newmerged_df.drop(col, axis=1)\n",
    "            \n",
    "        newmerged_df.to_csv('/home/kabank/workbench/.conda/analysis/kabank-data/result_data/Split_Full_Phecode_ATC.csv',index=False)\n",
    "        \n",
    "    \n",
    "class ClassifyReadmissionWithMedicationDiagnosis:\n",
    "    def __init__(self, file2):\n",
    "        self.file2 = file2\n",
    "        self.merged_df = None\n",
    "        self.selected_column_merged_df = None\n",
    "        #self.with_diag_medic_selected_column_merged_df = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        original_df_1 = pd.read_csv(self.file2)\n",
    "        self.merged_df = original_df_1[['num_diagnoses', 'num_medications', 'remaining_time_countdown', 'var_no_dates_permonth','Length_of_Episode', 'Count_visit', 'Therapy_ratio','Examination_ratio', 'Advisory_ratio',\n",
    "        'TreatmentPlanning_ratio','Outpatient_ratio','Inpatient_daynight_ratio', 'Care_intensity', 'closingcode_0', 'closingcode_1', 'closingcode_2', 'closingcode_3','closingcode_4', 'closingcode_5', 'closingcode_6', 'closingcode_9',\n",
    "        'aftercode_1', 'aftercode_2', 'aftercode_3', 'aftercode_4','aftercode_5', 'gender_0', 'F', 'M', 'MiddleChildhood', 'Preschooler','Teenager', 'TNE_NO_365','TNE_BO_365',\n",
    "        '010', '0703', '0704', '079', '0792', '1000', '1008', '1009', '1011', '1013', '1015', '136', '1499', '1551', '1701', '1702', '18911', '1911', '19111', '1994', '201', '2022', '20411', '20421', '2441', '2442', '2444', '2445', '24521',\n",
    "        '2501', '25011', '2502', '2531', '25311', '2532', '2553', '2564', '259', '260', '2602', '2612', '2614', '262', '2701', '2713', '27614', '2781', '2791', '2801', '28731', '288', '28811', '2903', '2911', '2914', '2922', '295', '2951',\n",
    "        '2952', '2953', '296', '2961', '29622', '2972', '300', '3001', '3003', '3004', '3008', '3009', '301', '3011', '3012', '302', '303', '3031', '3033', '3034', '304', '3052', '306', '3069', '312', '3123', '313', '3131', '3132', '3133',\n",
    "        '315', '3151', '3152', '3153', '316', '317', '323', '324', '327', '3274', '32741', '3275', '3276', '3311', '339', '340', '3401', '341', '343', '344', '345', '3451', '34511', '34512', '3453', '347', '3484', '357', '3591', '3592',\n",
    "        '3621', '365', '3671', '3672', '3678', '3679', '368', '3681', '3684', '3695', '3711', '3781', '3782', '3811', '389', '3891', '3892', '3894', '3895', '42711', '4332', '4338', '4589', '472', '4741', '4742', '476', '4801', '495', '496',\n",
    "        '499', '506', '5091', '510', '53011', '53014', '5302', '5305', '535', '5551', '5552', '55521', '558', '5611', '563', '564', '5641', '565', '57181', '5772', '591', '5921', '5965', '5994', '6041', '62611', '6264', '637', '6491', '656',\n",
    "        '6562', '6563', '6564', '657', '658', '661', '691', '6941', '69542', '6964', '7061', '71011', '7142', '7169', '7282', '7321', '733', '736', '7373', '739', '745', '7471', '74711', '74712', '74713', '7472', '748', '749', '7491', '7492',\n",
    "        '75011', '75013', '75021', '75112', '7512', '75121', '75211', '7522', '7531', '7551', '7554', '7556', '75561', '756', '7561', '758', '7581', '759', '760', '770', '773', '7981', '8003', '802', '8032', '8033', '805', '817', '818', '819',\n",
    "        '830', '835', '871', '907', '915', '930', '939', '946', '947', '949', '969', '979', '981', 'A02BA02', 'A02BC01', 'A02BC05', 'A03FA01', 'A06AD11', 'A06AD65', 'A06BA04', 'A07AA02', 'A10BA02', 'A11EA', 'A12AX', 'A12BA01', 'A12BA02', 'B03AA07',\n",
    "        'B03BA03', 'C02AC02', 'C07AA05', 'C09AA02', 'D01AC20', 'D06AA03', 'D06AX01', 'D06AX05', 'D06BB03', 'D07AA02', 'D07AB02', 'D07AB08', 'D07AC01', 'D07AC13', 'D07BC01', 'D07XC01', 'D09AA02', 'D10AD01', 'D10AD03', 'D10AD53', 'G03AA07', 'G03AA09',\n",
    "        'G03AA12', 'G03AA13', 'G03AC06', 'G03AC09', 'G03AD02', 'G03CA03', 'G03FB05', 'G03HB01', 'H01BA02', 'H02AB02', 'J01CA08', 'J01CE02', 'J01CF01', 'J01FA10', 'J05AB01', 'M01AE01', 'N01BB02', 'N01BB20', 'N02AA59', 'N02BA01', 'N02BE01', 'N02CC01',\n",
    "        'N02CC03', 'N02CX02', 'N03AE01', 'N03AF01', 'N03AG01', 'N03AX09', 'N03AX14', 'N05AA02', 'N05AF03', 'N05AH03', 'N05AH04', 'N05AX08', 'N05AX12', 'N05BA01', 'N05BA04', 'N05BB01', 'N05CD02', 'N05CF01', 'N05CF02', 'N05CH01', 'N06AA04', 'N06AA09',\n",
    "        'N06AB03', 'N06AB04', 'N06AB05', 'N06AB06', 'N06AB10', 'N06AX03', 'N06AX11', 'N06AX12', 'N06AX16', 'N06BA02', 'N06BA04', 'N06BA09', 'N06BA12', 'N07BA02', 'N07BA03', 'NO6BA04', 'NO6BAO4', 'NO6BAO9', 'R01AC02', 'R01AD09', 'R03AC03', 'R03BA02',\n",
    "        'R03BA05', 'R05DA01', 'R06AD01', 'R06AE07', 'R06AX27', 'S01AA01', 'S01AA13', 'S01GX02', 'S01GX09', 'nan']]\n",
    "        \n",
    "        # fill nan values in independent column\n",
    "        self.merged_df['num_diagnoses'].fillna(0, inplace=True)\n",
    "        self.merged_df['num_medications'].fillna(0, inplace=True)\n",
    "        self.merged_df['Outpatient_ratio'].fillna(0, inplace=True)\n",
    "        self.merged_df['Inpatient_daynight_ratio'].fillna(0, inplace=True)  \n",
    "        self.merged_df['Therapy_ratio'].fillna(0, inplace=True)\n",
    "        self.merged_df['Examination_ratio'].fillna(0, inplace=True)\n",
    "        self.merged_df['Advisory_ratio'].fillna(0, inplace=True)\n",
    "        self.merged_df['TreatmentPlanning_ratio'].fillna(0, inplace=True)\n",
    "        \n",
    "        \n",
    "        # define dependent variables\n",
    "        self.dependent_variable_TNE_BO_365 = self.merged_df[['TNE_BO_365']]\n",
    "        \n",
    "        self.independent_variable = self.merged_df[['num_diagnoses', 'num_medications', 'remaining_time_countdown', 'var_no_dates_permonth','Length_of_Episode', 'Count_visit', 'Therapy_ratio','Examination_ratio', 'Advisory_ratio',\n",
    "        'TreatmentPlanning_ratio','Outpatient_ratio','Inpatient_daynight_ratio', 'Care_intensity', 'closingcode_0', 'closingcode_1', 'closingcode_2', 'closingcode_3','closingcode_4', 'closingcode_5', 'closingcode_6', 'closingcode_9',\n",
    "        'aftercode_1', 'aftercode_2', 'aftercode_3', 'aftercode_4','aftercode_5', 'gender_0', 'F', 'M', 'MiddleChildhood', 'Preschooler','Teenager',\n",
    "        '010', '0703', '0704', '079', '0792', '1000', '1008', '1009', '1011', '1013', '1015', '136', '1499', '1551', '1701', '1702', '18911', '1911', '19111', '1994', '201', '2022', '20411', '20421', '2441', '2442', '2444', '2445', '24521',\n",
    "        '2501', '25011', '2502', '2531', '25311', '2532', '2553', '2564', '259', '260', '2602', '2612', '2614', '262', '2701', '2713', '27614', '2781', '2791', '2801', '28731', '288', '28811', '2903', '2911', '2914', '2922', '295', '2951',\n",
    "        '2952', '2953', '296', '2961', '29622', '2972', '300', '3001', '3003', '3004', '3008', '3009', '301', '3011', '3012', '302', '303', '3031', '3033', '3034', '304', '3052', '306', '3069', '312', '3123', '313', '3131', '3132', '3133',\n",
    "        '315', '3151', '3152', '3153', '316', '317', '323', '324', '327', '3274', '32741', '3275', '3276', '3311', '339', '340', '3401', '341', '343', '344', '345', '3451', '34511', '34512', '3453', '347', '3484', '357', '3591', '3592',\n",
    "        '3621', '365', '3671', '3672', '3678', '3679', '368', '3681', '3684', '3695', '3711', '3781', '3782', '3811', '389', '3891', '3892', '3894', '3895', '42711', '4332', '4338', '4589', '472', '4741', '4742', '476', '4801', '495', '496',\n",
    "        '499', '506', '5091', '510', '53011', '53014', '5302', '5305', '535', '5551', '5552', '55521', '558', '5611', '563', '564', '5641', '565', '57181', '5772', '591', '5921', '5965', '5994', '6041', '62611', '6264', '637', '6491', '656',\n",
    "        '6562', '6563', '6564', '657', '658', '661', '691', '6941', '69542', '6964', '7061', '71011', '7142', '7169', '7282', '7321', '733', '736', '7373', '739', '745', '7471', '74711', '74712', '74713', '7472', '748', '749', '7491', '7492',\n",
    "        '75011', '75013', '75021', '75112', '7512', '75121', '75211', '7522', '7531', '7551', '7554', '7556', '75561', '756', '7561', '758', '7581', '759', '760', '770', '773', '7981', '8003', '802', '8032', '8033', '805', '817', '818', '819',\n",
    "        '830', '835', '871', '907', '915', '930', '939', '946', '947', '949', '969', '979', '981', 'A02BA02', 'A02BC01', 'A02BC05', 'A03FA01', 'A06AD11', 'A06AD65', 'A06BA04', 'A07AA02', 'A10BA02', 'A11EA', 'A12AX', 'A12BA01', 'A12BA02', 'B03AA07',\n",
    "        'B03BA03', 'C02AC02', 'C07AA05', 'C09AA02', 'D01AC20', 'D06AA03', 'D06AX01', 'D06AX05', 'D06BB03', 'D07AA02', 'D07AB02', 'D07AB08', 'D07AC01', 'D07AC13', 'D07BC01', 'D07XC01', 'D09AA02', 'D10AD01', 'D10AD03', 'D10AD53', 'G03AA07', 'G03AA09',\n",
    "        'G03AA12', 'G03AA13', 'G03AC06', 'G03AC09', 'G03AD02', 'G03CA03', 'G03FB05', 'G03HB01', 'H01BA02', 'H02AB02', 'J01CA08', 'J01CE02', 'J01CF01', 'J01FA10', 'J05AB01', 'M01AE01', 'N01BB02', 'N01BB20', 'N02AA59', 'N02BA01', 'N02BE01', 'N02CC01',\n",
    "        'N02CC03', 'N02CX02', 'N03AE01', 'N03AF01', 'N03AG01', 'N03AX09', 'N03AX14', 'N05AA02', 'N05AF03', 'N05AH03', 'N05AH04', 'N05AX08', 'N05AX12', 'N05BA01', 'N05BA04', 'N05BB01', 'N05CD02', 'N05CF01', 'N05CF02', 'N05CH01', 'N06AA04', 'N06AA09',\n",
    "        'N06AB03', 'N06AB04', 'N06AB05', 'N06AB06', 'N06AB10', 'N06AX03', 'N06AX11', 'N06AX12', 'N06AX16', 'N06BA02', 'N06BA04', 'N06BA09', 'N06BA12', 'N07BA02', 'N07BA03', 'NO6BA04', 'NO6BAO4', 'NO6BAO9', 'R01AC02', 'R01AD09', 'R03AC03', 'R03BA02',\n",
    "        'R03BA05', 'R05DA01', 'R06AD01', 'R06AE07', 'R06AX27', 'S01AA01', 'S01AA13', 'S01GX02', 'S01GX09']]\n",
    "        \n",
    "        return self.merged_df, self.dependent_variable_TNE_BO_365, self.independent_variable \n",
    "    \n",
    "    # To train and view the reasult of binary and multiclass classifier        \n",
    "    def train_classifier_with_medication_diagnosis(self):\n",
    "        dependent_variables = [self.dependent_variable_TNE_BO_365]\n",
    "        \n",
    "        # to define the weight of output class, to resolve imbalanced output/dataset\n",
    "        dependent_variable_names = {'self.dependent_variable_TNE_BO_365': 'TNE_BO_365'}\n",
    "        class_weights = {'TNE_BO_365': {0:10, 1:254}}\n",
    "              \n",
    "        #for dependent_variable in dependent_variables:\n",
    "        for variable_name, dependent_variable in zip(dependent_variable_names.values(), dependent_variables):\n",
    "            logistic_prediction_model = LogisticRegression(class_weight=class_weights[variable_name])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(self.independent_variable, dependent_variable, train_size=0.7)\n",
    "            logistic_prediction_model.fit(X_train,y_train)\n",
    "            y_pred = logistic_prediction_model.predict(X_test)\n",
    "            \n",
    "            # Checks if the output category for classification is binary or multiclass\n",
    "            category_count = int(y_train.nunique())\n",
    "            print(f'\\n****** Evaluation result {variable_name} as dependent variable ******')\n",
    "            # For binary class classification \n",
    "            if category_count == 2:\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='binary')\n",
    "                recall = recall_score(y_test, y_pred, average='binary')\n",
    "                f1 = f1_score(y_test, y_pred, average='binary')\n",
    "                \n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                print(f\"Precision: {precision}\")\n",
    "                print(f\"Recall: {recall}\")\n",
    "                print(f\"F1score: {f1}\")\n",
    "                print(\"Confused matrix:\")\n",
    "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                tn, fp, fn, tp = conf_matrix.ravel()\n",
    "                print(conf_matrix)\n",
    "                print(f\"True Negative (TN): {tn}\")\n",
    "                print(f\"False Positive (FP): {fp}\")\n",
    "                print(f\"False Negative (FN): {fn}\")\n",
    "                print(f\"True Positive (TP): {tp}\")\n",
    "                print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            # For binary class classification \n",
    "            else:\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                print(f\"Precision: {precision}\")\n",
    "                print(f\"Recall: {recall}\")\n",
    "                print(f\"F1score: {f1}\")\n",
    "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                print(\"Confused matrix:\")\n",
    "                print(conf_matrix)\n",
    "                print(classification_report(y_test, y_pred))\n",
    "                \n",
    "    def prediction_with_medication_diagnosis(self):\n",
    "        # Select all corresponding columns with values in TNE_NO_365\n",
    "        self.merged_df = self.merged_df.dropna(subset=['TNE_NO_365'])\n",
    "        \n",
    "        # define dependent variables\n",
    "        self.dependent_variable_TNE_NO_365 = self.merged_df[['TNE_NO_365']]\n",
    "        # define independent variables \n",
    "        self.independent_variable = self.merged_df[['num_diagnoses', 'num_medications', 'remaining_time_countdown', 'var_no_dates_permonth','Length_of_Episode', 'Count_visit', 'Therapy_ratio','Examination_ratio', 'Advisory_ratio',\n",
    "        'TreatmentPlanning_ratio','Outpatient_ratio','Inpatient_daynight_ratio', 'Care_intensity', 'closingcode_0', 'closingcode_1', 'closingcode_2', 'closingcode_3','closingcode_4', 'closingcode_5', 'closingcode_6', 'closingcode_9',\n",
    "        'aftercode_1', 'aftercode_2', 'aftercode_3', 'aftercode_4','aftercode_5', 'gender_0', 'F', 'M', 'MiddleChildhood', 'Preschooler','Teenager',\n",
    "        '010', '0703', '0704', '079', '0792', '1000', '1008', '1009', '1011', '1013', '1015', '136', '1499', '1551', '1701', '1702', '18911', '1911', '19111', '1994', '201', '2022', '20411', '20421', '2441', '2442', '2444', '2445', '24521',\n",
    "        '2501', '25011', '2502', '2531', '25311', '2532', '2553', '2564', '259', '260', '2602', '2612', '2614', '262', '2701', '2713', '27614', '2781', '2791', '2801', '28731', '288', '28811', '2903', '2911', '2914', '2922', '295', '2951',\n",
    "        '2952', '2953', '296', '2961', '29622', '2972', '300', '3001', '3003', '3004', '3008', '3009', '301', '3011', '3012', '302', '303', '3031', '3033', '3034', '304', '3052', '306', '3069', '312', '3123', '313', '3131', '3132', '3133',\n",
    "        '315', '3151', '3152', '3153', '316', '317', '323', '324', '327', '3274', '32741', '3275', '3276', '3311', '339', '340', '3401', '341', '343', '344', '345', '3451', '34511', '34512', '3453', '347', '3484', '357', '3591', '3592',\n",
    "        '3621', '365', '3671', '3672', '3678', '3679', '368', '3681', '3684', '3695', '3711', '3781', '3782', '3811', '389', '3891', '3892', '3894', '3895', '42711', '4332', '4338', '4589', '472', '4741', '4742', '476', '4801', '495', '496',\n",
    "        '499', '506', '5091', '510', '53011', '53014', '5302', '5305', '535', '5551', '5552', '55521', '558', '5611', '563', '564', '5641', '565', '57181', '5772', '591', '5921', '5965', '5994', '6041', '62611', '6264', '637', '6491', '656',\n",
    "        '6562', '6563', '6564', '657', '658', '661', '691', '6941', '69542', '6964', '7061', '71011', '7142', '7169', '7282', '7321', '733', '736', '7373', '739', '745', '7471', '74711', '74712', '74713', '7472', '748', '749', '7491', '7492',\n",
    "        '75011', '75013', '75021', '75112', '7512', '75121', '75211', '7522', '7531', '7551', '7554', '7556', '75561', '756', '7561', '758', '7581', '759', '760', '770', '773', '7981', '8003', '802', '8032', '8033', '805', '817', '818', '819',\n",
    "        '830', '835', '871', '907', '915', '930', '939', '946', '947', '949', '969', '979', '981', 'A02BA02', 'A02BC01', 'A02BC05', 'A03FA01', 'A06AD11', 'A06AD65', 'A06BA04', 'A07AA02', 'A10BA02', 'A11EA', 'A12AX', 'A12BA01', 'A12BA02', 'B03AA07',\n",
    "        'B03BA03', 'C02AC02', 'C07AA05', 'C09AA02', 'D01AC20', 'D06AA03', 'D06AX01', 'D06AX05', 'D06BB03', 'D07AA02', 'D07AB02', 'D07AB08', 'D07AC01', 'D07AC13', 'D07BC01', 'D07XC01', 'D09AA02', 'D10AD01', 'D10AD03', 'D10AD53', 'G03AA07', 'G03AA09',\n",
    "        'G03AA12', 'G03AA13', 'G03AC06', 'G03AC09', 'G03AD02', 'G03CA03', 'G03FB05', 'G03HB01', 'H01BA02', 'H02AB02', 'J01CA08', 'J01CE02', 'J01CF01', 'J01FA10', 'J05AB01', 'M01AE01', 'N01BB02', 'N01BB20', 'N02AA59', 'N02BA01', 'N02BE01', 'N02CC01',\n",
    "        'N02CC03', 'N02CX02', 'N03AE01', 'N03AF01', 'N03AG01', 'N03AX09', 'N03AX14', 'N05AA02', 'N05AF03', 'N05AH03', 'N05AH04', 'N05AX08', 'N05AX12', 'N05BA01', 'N05BA04', 'N05BB01', 'N05CD02', 'N05CF01', 'N05CF02', 'N05CH01', 'N06AA04', 'N06AA09',\n",
    "        'N06AB03', 'N06AB04', 'N06AB05', 'N06AB06', 'N06AB10', 'N06AX03', 'N06AX11', 'N06AX12', 'N06AX16', 'N06BA02', 'N06BA04', 'N06BA09', 'N06BA12', 'N07BA02', 'N07BA03', 'NO6BA04', 'NO6BAO4', 'NO6BAO9', 'R01AC02', 'R01AD09', 'R03AC03', 'R03BA02',\n",
    "        'R03BA05', 'R05DA01', 'R06AD01', 'R06AE07', 'R06AX27', 'S01AA01', 'S01AA13', 'S01GX02', 'S01GX09']]\n",
    "        \n",
    "        dependent_variables = [self.dependent_variable_TNE_NO_365]\n",
    "        dependent_variable_names = {'self.dependent_variable_TNE_NO_365':'TNE_NO_365'}\n",
    "        #for dependent_variable in dependent_variables:\n",
    "        for variable_name, dependent_variable in zip(dependent_variable_names.values(), dependent_variables):\n",
    "            linear_prediction_model = LinearRegression()\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(self.independent_variable, dependent_variable, train_size=0.7)\n",
    "            X_test,X_eval,y_test,y_eval = train_test_split(X_temp,y_temp,test_size=0.33)\n",
    "            linear_prediction_model.fit(X_train,y_train)\n",
    "            y_pred = linear_prediction_model.predict(X_test)\n",
    "            print(f'\\n****** Prediction Model Evaluation result  {variable_name} as dependent variable ******')\n",
    "            y_pred = linear_prediction_model.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            print('R-squared:', r2)\n",
    "            print('Mean squared error:', mse)\n",
    "                \n",
    "\n",
    "#SplitMedicationDiagnosisInUniquePieces_Obj = SplitMedicationDiagnosisInUniquePieces(os.getenv(\"FILE_Full_Phecode_ATC_PATH\"))\n",
    "SplitMedicationDiagnosisInUniquePieces_Obj = SplitMedicationDiagnosisInUniquePieces(\"/home/kabank/workbench/.conda/analysis/kabank-data/new-data/Full_Phecode_ATC.csv\")\n",
    "SplitMedicationDiagnosisInUniquePieces_Obj.split_diagnosis_medication_in_unique_pieces()\n",
    "\n",
    "ClassifyReadmissionWithMedicationDiagnosis_Obj = ClassifyReadmissionWithMedicationDiagnosis('/home/kabank/workbench/.conda/analysis/kabank-data/result_data/Split_Full_Phecode_ATC.csv')\n",
    "merged_df, dependent_variable_TNE_BO_365, independent_variable  = ClassifyReadmissionWithMedicationDiagnosis_Obj.load_data()\n",
    "ClassifyReadmissionWithMedicationDiagnosis_Obj.train_classifier_with_medication_diagnosis()\n",
    "ClassifyReadmissionWithMedicationDiagnosis_Obj.prediction_with_medication_diagnosis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Kernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
