{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9947eea4-1f70-40a5-bdbd-eec947e8abdf",
   "metadata": {},
   "source": [
    "#### Task: Classification and Predicition of readmission with medication and diagnosis columns, based on ATC codes\n",
    "**`TNE_BO_180` values (0 and 1 are used to classify readmission, where 1 means readmission). Further for prediction only the readmitted patient data is taken and a linear regression model is trained on it to predict the number of days for readmission.**\n",
    "- Dataset Used: `/mnt/work/workbench/dipendrp/new-data/Full_ICD10_ATC.csv`\n",
    "- Independent variable: `'age', 'remaining_time_countdown','var_no_dates_permonth', 'gender','closingcode', 'aftercode', 'Length_of_Episode', \n",
    "                                                   'Count_visit','Therapy_ratio', 'Examination_ratio', 'Advisory_ratio',\n",
    "                                                   'TreatmentPlanning_ratio', 'Outpatient_ratio','Inpatient_daynight_ratio', 'Care_intensity', 'num_diagnoses', 'num_medications'`\n",
    "- Dependent Variable: For classification its `TNE_BO_180` and for prediction its `TNE_NO_180` \n",
    "- Other Important Info: Dataset has NaN values. NaN has been replaced with 0\n",
    "\n",
    "<br>\n",
    "Description of work done:\n",
    "\n",
    "1. In this scripts the first class `SplitMedicationDiagnosisInUniquePieces` splits all the diagnosia and medication column into unique pieces.\n",
    "2. Then it saves the new data frame with unique diagnosis and medication as new `CSV` file named as `Split_Full_ICD10_ATC`\n",
    "3. Then the second class `ClassifyReadmissionWithMedicationDiagnosis` takes the new `Split_Full_ICD10_ATC` dataset and perform classiciation and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d678b9-83cc-485e-8572-6ed9ed9d9558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "######### Split Medication and Diagnosis In Unique Pieces ############\n",
    "#                                                                    #\n",
    "#                                                                    #\n",
    "#                                                                    #\n",
    "######################################################################\n",
    "\n",
    "class SplitMedicationDiagnosisInUniquePieces:\n",
    "    def __init__(self, file1):\n",
    "        self.file1 = file1\n",
    "        self.merged_df = None\n",
    "        self.selected_column_merged_df = None\n",
    "        self.without_diag_medic_selected_column_merged_df = None\n",
    "    \n",
    "    def load_data_to_split(self):\n",
    "        Full_ICD10_ATC = pd.read_csv(self.file1)\n",
    "        self.merged_df = Full_ICD10_ATC \n",
    "        self.merged_df = self.merged_df[['episode_id', 'num_diagnoses', 'num_medications', 'pasient', 'age',\n",
    "                                       'remaining_time_countdown', 'var_no_dates_permonth', 'gender',\n",
    "                                       'episode_order', 'islast', 'closingcode', 'aftercode',\n",
    "                                       'episode_start_date', 'episode_end_date', 'tillnextepisode',\n",
    "                                       'Length_of_Episode', 'Cat_LOE', 'TNE_BO_180', 'TNE_NO_180',\n",
    "                                       'TNE_BO_365', 'TNE_NO_365', 'TNE_BO_730', 'TNE_NO_730', 'TNE_BO_1095',\n",
    "                                       'TNE_NO_1095', 'Cat_LOE_desc', 'Count_visit', 'Cat_CV', 'Therapy_ratio',\n",
    "                                       'Examination_ratio', 'Advisory_ratio', 'TreatmentPlanning_ratio',\n",
    "                                       'Outpatient_ratio', 'Inpatient_ratio', 'Inpatient_day_ratio',\n",
    "                                       'Inpatient_daynight_ratio', 'Care_intensity', 'age_group',\n",
    "                                       'closingcode_0', 'closingcode_1', 'closingcode_2', 'closingcode_3',\n",
    "                                       'closingcode_4', 'closingcode_5', 'closingcode_6', 'closingcode_9',\n",
    "                                       'aftercode_1', 'aftercode_2', 'aftercode_3', 'aftercode_4',\n",
    "                                       'aftercode_5', 'gender_0', 'F', 'M', 'MiddleChildhood', 'Preschooler',\n",
    "                                       'Teenager', 'diagnoses', 'actual_med_Full_ATC']]\n",
    "        \n",
    "        self.merged_df = self.merged_df.copy(deep=True)\n",
    "        return self.merged_df\n",
    "\n",
    "        \n",
    "    def split_diagnosis_medication_in_unique_pieces(self):\n",
    "        newmerged_df = SplitMedicationDiagnosisInUniquePieces_Obj.load_data_to_split()\n",
    "        columns = ['diagnoses', 'actual_med_Full_ATC']\n",
    "        for col in columns:\n",
    "            newmerged_df[col] = newmerged_df[col].apply(lambda d:[] if pd.isnull(d) else d)\n",
    "            newmerged_df[col] = newmerged_df[col].str.replace(\"[\", \"\")\n",
    "            newmerged_df[col] = newmerged_df[col].str.replace(\"]\", \"\")\n",
    "            newmerged_df[col] = newmerged_df[col].str.replace(\"'\", \"\")\n",
    "            newmerged_df[col] = newmerged_df[col].str.replace(\" \", \"\")\n",
    "            newmerged_df[col] = newmerged_df[col].str.split(',')\n",
    "            newmerged_df = newmerged_df.explode(col)\n",
    "            \n",
    "            # Show only top 100 columns names\n",
    "            temp_store_100 = newmerged_df[col].value_counts()\n",
    "            names_only = temp_store_100.index.tolist()\n",
    "            \n",
    "            original_columns = newmerged_df.columns.tolist()\n",
    "            dummies = newmerged_df[col].str.get_dummies(sep=',')\n",
    "            newmerged_df = pd.concat([newmerged_df, dummies], axis=1)\n",
    "            newmerged_df = newmerged_df.drop(col, axis=1)\n",
    "            \n",
    "        newmerged_df.to_csv('Split_Full_ICD10_ATC.csv',index=False)\n",
    "        \n",
    "    \n",
    "class ClassifyReadmissionWithMedicationDiagnosis:\n",
    "    def __init__(self, file2):\n",
    "        self.file2 = file2\n",
    "        self.merged_df = None\n",
    "        self.selected_column_merged_df = None\n",
    "        self.without_diag_medic_selected_column_merged_df = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        original_df_1 = pd.read_csv(self.file2)\n",
    "        self.merged_df = original_df_1[['num_diagnoses', 'num_medications', 'remaining_time_countdown', 'var_no_dates_permonth','Length_of_Episode', 'Count_visit', 'Therapy_ratio','Examination_ratio', 'Advisory_ratio',\n",
    "        'TreatmentPlanning_ratio','Outpatient_ratio','Inpatient_daynight_ratio', 'Care_intensity', 'closingcode_0', 'closingcode_1', 'closingcode_2', 'closingcode_3','closingcode_4', 'closingcode_5', 'closingcode_6', 'closingcode_9',\n",
    "        'aftercode_1', 'aftercode_2', 'aftercode_3', 'aftercode_4','aftercode_5', 'gender_0', 'F', 'M', 'MiddleChildhood', 'Preschooler','Teenager', 'TNE_NO_180','TNE_BO_180',\n",
    "        '3131', '29622', '3151', '313', '3001', '304', '312', '3132', '3133', '3009', '3052', '2781', '3003', '2972', '3152', '3153', '2501', '1019', '563', '315', '495', '75021', '7981', '637', '1004', '476', '798', '939', '3034', '345', \n",
    "        '339', '3004', '340', '969', '5994', '7142', '2926', '3069', '785', '3008', '3891', '781', '316', '343', '2961', '20411', '2953', '561', '3012', '2903', '915', '300', '3689', '2951', '317', '3031', '949', '19111', '1002', '3781', \n",
    "        '789', '3453', '74711', '661', '301', '3401', '3678', '3679', '3671', '327', '2911', '296', '7522', '7581', '3276', '3033', '74712', '34512', '3011', '7373', '2713', '389', '3591', '1702', '3123', '3274', '75211', '2649', '2444', \n",
    "        '74713', '6562', '770', '871', '819', '302', '34511', '3711', '1008', '5552', '5551', '3811', '2801', '2952', '788', '2914', '3451', '733', '7512', '656', '368', '75112', '62611', '658', '3311', '760', '7551', '1994', '2022', '277', \n",
    "        '1015', '262', '745', '201', '5091', '3895', '2532', '306', '1551', '292', '7554', '5641', '3892', '7321', '1023', '759', '53011', '2445', '18911', '323', '7282', '69542', '5134', '930', '20421', '3672', '7561', '2791', '3869', '0704',\n",
    "        '2442', '979', '758', '3592', '496', '2643', '3695', '736','3381', '75121', '7061', '565', '2612', '24521', '1009', '773', '499', '357', '57181', '1031', '947', '347', '079', '3501', '136', '2924', '5965', '1028', '55521', '42711', '1701', \n",
    "        '835', '1011', '830', '0792', '28731', '6964', '5611', '75561', '1000', '506', '27614', '3684', '818', '6563', '748', '3275', '1029', '53014', '3382', '5302', '472', '295', '535', '32741', '25311', '324', '3894', '2553', '4181', '817', \n",
    "        '341', '6041', '7472', '739', '75011', '981', '1911', '2564', '691', '6564', '25011', '303', '756', '2441', '6941', '1033', '3782', '7491', '5772', '3462', '802', '344', '3681', '8003', '749', '558', '3484', '6264', '0703', '4332', \n",
    "        '7531', '1034', '564', '3621', '2531', '365','396', '4589', '7471', '1010', '6491', '7492', '1499', '288', '5921', '4801', '260', '7556', '1013', '4742', '4338', '805', '2701', '259', '28811', '907', '8032', '8033', '591', '532', '946', \n",
    "        '2768', '2602', '5305', '2921', '010', '510', '71011', '4741', '657', '75013', '2922', '2502', '2614', '7169','N06BA04', 'N06AB06', 'N06BA09', 'A06BA04', 'N06AB03', 'R06AD01', 'N05CH01', 'N05AX08', 'N05AH04', 'N05AX12', 'N06BA12', 'N03AX09', \n",
    "        'NO6BAO4', 'N06AB10', 'N05AH03','NO6BA04', 'N05AA02', 'N06AX03', 'N05CF01', 'A11EA', 'A12AX', 'N05BA01', 'N06AX16', 'N03AG01', 'N05AF03', 'N02CX02', 'N05CF02', 'N06AA04', 'R06AE07', 'G03AA09', 'N02CC03', 'G03AA07', 'N03AE01', 'G03AC09', \n",
    "        'A10BA02', 'H01BA02', 'B03AA07','N06AB04', 'A02BC05', 'N06AX11', 'N05BB01', 'N06AB05', 'N05CD02', 'N05BA04', 'J01FA10', 'D07AC13', 'N01BB20', 'A06AD65', 'A03FA01', 'N06BA02', 'D10AD03', 'G03AA12', 'G03AA13', 'D07BC01', 'S01GX02', 'A02BA02', \n",
    "        'G03AC06', 'NO6BAO9', 'N02CC01', 'N02AA59', 'J01CF01', 'D07AA02', 'R01AD09', 'D07AC01', 'N03AX14', 'G03FB05', 'N03AF01', 'A12BA02', 'D07AB08', 'A06AD11', 'G03AD02', 'S01AA01', 'N02BA01', 'D01AC20', 'R01AC02', 'M01AE01', 'S01GX09', 'A02BC01', \n",
    "        'A12BA01', 'C02AC02', 'D07XC01', 'H02AB02', 'N02BE01', 'R05DA01', 'D06AX01', 'S01AA13', 'J05AB01', 'C09AA02', 'A07AA02', 'D06AX05', 'D10AD53', 'D10AD01', 'N07BA03', 'C07AA05', 'G03CA03', 'N06AX12', 'J01CA08', 'N07BA02', 'D06AA03', 'D07AB02', \n",
    "        'R03BA05', 'G03HB01', 'B03BA03', 'D06BB03', 'R06AX27', 'N01BB02', 'R03BA02', 'R03AC03', 'J01CE02', 'D09AA02', 'N06AA09']]\n",
    "        \n",
    "        \n",
    "       \n",
    "        # fill nan values in independent column\n",
    "        self.merged_df['num_diagnoses'].fillna(0, inplace=True)\n",
    "        self.merged_df['num_medications'].fillna(0, inplace=True)\n",
    "        self.merged_df['Outpatient_ratio'].fillna(0, inplace=True)\n",
    "        self.merged_df['Inpatient_daynight_ratio'].fillna(0, inplace=True)  \n",
    "        self.merged_df['Therapy_ratio'].fillna(0, inplace=True)\n",
    "        self.merged_df['Examination_ratio'].fillna(0, inplace=True)\n",
    "        self.merged_df['Advisory_ratio'].fillna(0, inplace=True)\n",
    "        self.merged_df['TreatmentPlanning_ratio'].fillna(0, inplace=True)\n",
    "        \n",
    "        \n",
    "        # define dependent variables\n",
    "        self.dependent_variable_TNE_BO_180 = self.merged_df[['TNE_BO_180']]\n",
    "        \n",
    "        self.independent_variable = self.merged_df[['num_diagnoses', 'num_medications', 'remaining_time_countdown', 'var_no_dates_permonth','Length_of_Episode', 'Count_visit', 'Therapy_ratio','Examination_ratio', 'Advisory_ratio',\n",
    "        'TreatmentPlanning_ratio','Outpatient_ratio','Inpatient_daynight_ratio', 'Care_intensity', 'closingcode_0', 'closingcode_1', 'closingcode_2', 'closingcode_3','closingcode_4', 'closingcode_5', 'closingcode_6', 'closingcode_9',\n",
    "        'aftercode_1', 'aftercode_2', 'aftercode_3', 'aftercode_4','aftercode_5', 'gender_0', 'F', 'M', 'MiddleChildhood', 'Preschooler','Teenager',\n",
    "        '3131', '29622', '3151', '313', '3001', '304', '312', '3132', '3133', '3009', '3052', '2781', '3003', '2972', '3152', '3153', '2501', '1019', '563', '315', '495', '75021', '7981', '637', '1004', '476', '798', '939', '3034', '345', \n",
    "        '339', '3004', '340', '969', '5994', '7142', '2926', '3069', '785', '3008', '3891', '781', '316', '343', '2961', '20411', '2953', '561', '3012', '2903', '915', '300', '3689', '2951', '317', '3031', '949', '19111', '1002', '3781', \n",
    "        '789', '3453', '74711', '661', '301', '3401', '3678', '3679', '3671', '327', '2911', '296', '7522', '7581', '3276', '3033', '74712', '34512', '3011', '7373', '2713', '389', '3591', '1702', '3123', '3274', '75211', '2649', '2444', \n",
    "        '74713', '6562', '770', '871', '819', '302', '34511', '3711', '1008', '5552', '5551', '3811', '2801', '2952', '788', '2914', '3451', '733', '7512', '656', '368', '75112', '62611', '658', '3311', '760', '7551', '1994', '2022', '277', \n",
    "        '1015', '262', '745', '201', '5091', '3895', '2532', '306', '1551', '292', '7554', '5641', '3892', '7321', '1023', '759', '53011', '2445', '18911', '323', '7282', '69542', '5134', '930', '20421', '3672', '7561', '2791', '3869', '0704',\n",
    "        '2442', '979', '758', '3592', '496', '2643', '3695', '736','3381', '75121', '7061', '565', '2612', '24521', '1009', '773', '499', '357', '57181', '1031', '947', '347', '079', '3501', '136', '2924', '5965', '1028', '55521', '42711', '1701', \n",
    "        '835', '1011', '830', '0792', '28731', '6964', '5611', '75561', '1000', '506', '27614', '3684', '818', '6563', '748', '3275', '1029', '53014', '3382', '5302', '472', '295', '535', '32741', '25311', '324', '3894', '2553', '4181', '817', \n",
    "        '341', '6041', '7472', '739', '75011', '981', '1911', '2564', '691', '6564', '25011', '303', '756', '2441', '6941', '1033', '3782', '7491', '5772', '3462', '802', '344', '3681', '8003', '749', '558', '3484', '6264', '0703', '4332', \n",
    "        '7531', '1034', '564', '3621', '2531', '365','396', '4589', '7471', '1010', '6491', '7492', '1499', '288', '5921', '4801', '260', '7556', '1013', '4742', '4338', '805', '2701', '259', '28811', '907', '8032', '8033', '591', '532', '946', \n",
    "        '2768', '2602', '5305', '2921', '010', '510', '71011', '4741', '657', '75013', '2922', '2502', '2614', '7169','N06BA04', 'N06AB06', 'N06BA09', 'A06BA04', 'N06AB03', 'R06AD01', 'N05CH01', 'N05AX08', 'N05AH04', 'N05AX12', 'N06BA12', 'N03AX09', \n",
    "        'NO6BAO4', 'N06AB10', 'N05AH03','NO6BA04', 'N05AA02', 'N06AX03', 'N05CF01', 'A11EA', 'A12AX', 'N05BA01', 'N06AX16', 'N03AG01', 'N05AF03', 'N02CX02', 'N05CF02', 'N06AA04', 'R06AE07', 'G03AA09', 'N02CC03', 'G03AA07', 'N03AE01', 'G03AC09', \n",
    "        'A10BA02', 'H01BA02', 'B03AA07','N06AB04', 'A02BC05', 'N06AX11', 'N05BB01', 'N06AB05', 'N05CD02', 'N05BA04', 'J01FA10', 'D07AC13', 'N01BB20', 'A06AD65', 'A03FA01', 'N06BA02', 'D10AD03', 'G03AA12', 'G03AA13', 'D07BC01', 'S01GX02', 'A02BA02', \n",
    "        'G03AC06', 'NO6BAO9', 'N02CC01', 'N02AA59', 'J01CF01', 'D07AA02', 'R01AD09', 'D07AC01', 'N03AX14', 'G03FB05', 'N03AF01', 'A12BA02', 'D07AB08', 'A06AD11', 'G03AD02', 'S01AA01', 'N02BA01', 'D01AC20', 'R01AC02', 'M01AE01', 'S01GX09', 'A02BC01', \n",
    "        'A12BA01', 'C02AC02', 'D07XC01', 'H02AB02', 'N02BE01', 'R05DA01', 'D06AX01', 'S01AA13', 'J05AB01', 'C09AA02', 'A07AA02', 'D06AX05', 'D10AD53', 'D10AD01', 'N07BA03', 'C07AA05', 'G03CA03', 'N06AX12', 'J01CA08', 'N07BA02', 'D06AA03', 'D07AB02', \n",
    "        'R03BA05', 'G03HB01', 'B03BA03', 'D06BB03', 'R06AX27', 'N01BB02', 'R03BA02', 'R03AC03', 'J01CE02', 'D09AA02', 'N06AA09']]\n",
    "        \n",
    "        return self.merged_df, self.dependent_variable_TNE_BO_180, self.independent_variable \n",
    "    \n",
    "    # To train and view the reasult of binary and multiclass classifier        \n",
    "    def train_classifier_with_medication_diagnosis(self):\n",
    "        dependent_variables = [self.dependent_variable_TNE_BO_180]\n",
    "        \n",
    "        # to define the weight of output class, to resolve imbalanced output/dataset\n",
    "        dependent_variable_names = {'self.dependent_variable_TNE_BO_180': 'TNE_BO_180'}\n",
    "        class_weights = {'TNE_BO_180': {0:10, 1:270}}\n",
    "              \n",
    "        #for dependent_variable in dependent_variables:\n",
    "        for variable_name, dependent_variable in zip(dependent_variable_names.values(), dependent_variables):\n",
    "            logistic_prediction_model = LogisticRegression(class_weight=class_weights[variable_name])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(self.independent_variable, dependent_variable, train_size=0.7)\n",
    "            logistic_prediction_model.fit(X_train,y_train)\n",
    "            y_pred = logistic_prediction_model.predict(X_test)\n",
    "            \n",
    "            # Checks if the output category for classification is binary or multiclass\n",
    "            category_count = int(y_train.nunique())\n",
    "            print(f'\\n****** Evaluation result {variable_name} as dependent variable ******')\n",
    "            # For binary class classification \n",
    "            if category_count == 2:\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='binary')\n",
    "                recall = recall_score(y_test, y_pred, average='binary')\n",
    "                f1 = f1_score(y_test, y_pred, average='binary')\n",
    "                \n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                print(f\"Precision: {precision}\")\n",
    "                print(f\"Recall: {recall}\")\n",
    "                print(f\"F1score: {f1}\")\n",
    "                print(\"Confused matrix:\")\n",
    "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                tn, fp, fn, tp = conf_matrix.ravel()\n",
    "                print(conf_matrix)\n",
    "                print(f\"True Negative (TN): {tn}\")\n",
    "                print(f\"False Positive (FP): {fp}\")\n",
    "                print(f\"False Negative (FN): {fn}\")\n",
    "                print(f\"True Positive (TP): {tp}\")\n",
    "                print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            # For binary class classification \n",
    "            else:\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                print(f\"Precision: {precision}\")\n",
    "                print(f\"Recall: {recall}\")\n",
    "                print(f\"F1score: {f1}\")\n",
    "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                print(\"Confused matrix:\")\n",
    "                print(conf_matrix)\n",
    "                print(classification_report(y_test, y_pred))\n",
    "                \n",
    "    def prediction_with_medication_diagnosis(self):\n",
    "        # Select all corresponding columns with values in TNE_NO_180\n",
    "        self.merged_df = self.merged_df.dropna(subset=['TNE_NO_180'])\n",
    "        \n",
    "        # define dependent variables\n",
    "        self.dependent_variable_TNE_NO_180 = self.merged_df[['TNE_NO_180']]\n",
    "        # define independent variables \n",
    "        self.independent_variable = self.merged_df[['num_diagnoses', 'num_medications', 'remaining_time_countdown', 'var_no_dates_permonth','Length_of_Episode', 'Count_visit', 'Therapy_ratio','Examination_ratio', 'Advisory_ratio',\n",
    "        'TreatmentPlanning_ratio','Outpatient_ratio','Inpatient_daynight_ratio', 'Care_intensity', 'closingcode_0', 'closingcode_1', 'closingcode_2', 'closingcode_3','closingcode_4', 'closingcode_5', 'closingcode_6', 'closingcode_9',\n",
    "        'aftercode_1', 'aftercode_2', 'aftercode_3', 'aftercode_4','aftercode_5', 'gender_0', 'F', 'M', 'MiddleChildhood', 'Preschooler','Teenager',\n",
    "        '3131', '29622', '3151', '313', '3001', '304', '312', '3132', '3133', '3009', '3052', '2781', '3003', '2972', '3152', '3153', '2501', '1019', '563', '315', '495', '75021', '7981', '637', '1004', '476', '798', '939', '3034', '345', \n",
    "        '339', '3004', '340', '969', '5994', '7142', '2926', '3069', '785', '3008', '3891', '781', '316', '343', '2961', '20411', '2953', '561', '3012', '2903', '915', '300', '3689', '2951', '317', '3031', '949', '19111', '1002', '3781', \n",
    "        '789', '3453', '74711', '661', '301', '3401', '3678', '3679', '3671', '327', '2911', '296', '7522', '7581', '3276', '3033', '74712', '34512', '3011', '7373', '2713', '389', '3591', '1702', '3123', '3274', '75211', '2649', '2444', \n",
    "        '74713', '6562', '770', '871', '819', '302', '34511', '3711', '1008', '5552', '5551', '3811', '2801', '2952', '788', '2914', '3451', '733', '7512', '656', '368', '75112', '62611', '658', '3311', '760', '7551', '1994', '2022', '277', \n",
    "        '1015', '262', '745', '201', '5091', '3895', '2532', '306', '1551', '292', '7554', '5641', '3892', '7321', '1023', '759', '53011', '2445', '18911', '323', '7282', '69542', '5134', '930', '20421', '3672', '7561', '2791', '3869', '0704',\n",
    "        '2442', '979', '758', '3592', '496', '2643', '3695', '736','3381', '75121', '7061', '565', '2612', '24521', '1009', '773', '499', '357', '57181', '1031', '947', '347', '079', '3501', '136', '2924', '5965', '1028', '55521', '42711', '1701', \n",
    "        '835', '1011', '830', '0792', '28731', '6964', '5611', '75561', '1000', '506', '27614', '3684', '818', '6563', '748', '3275', '1029', '53014', '3382', '5302', '472', '295', '535', '32741', '25311', '324', '3894', '2553', '4181', '817', \n",
    "        '341', '6041', '7472', '739', '75011', '981', '1911', '2564', '691', '6564', '25011', '303', '756', '2441', '6941', '1033', '3782', '7491', '5772', '3462', '802', '344', '3681', '8003', '749', '558', '3484', '6264', '0703', '4332', \n",
    "        '7531', '1034', '564', '3621', '2531', '365','396', '4589', '7471', '1010', '6491', '7492', '1499', '288', '5921', '4801', '260', '7556', '1013', '4742', '4338', '805', '2701', '259', '28811', '907', '8032', '8033', '591', '532', '946', \n",
    "        '2768', '2602', '5305', '2921', '010', '510', '71011', '4741', '657', '75013', '2922', '2502', '2614', '7169','N06BA04', 'N06AB06', 'N06BA09', 'A06BA04', 'N06AB03', 'R06AD01', 'N05CH01', 'N05AX08', 'N05AH04', 'N05AX12', 'N06BA12', 'N03AX09', \n",
    "        'NO6BAO4', 'N06AB10', 'N05AH03','NO6BA04', 'N05AA02', 'N06AX03', 'N05CF01', 'A11EA', 'A12AX', 'N05BA01', 'N06AX16', 'N03AG01', 'N05AF03', 'N02CX02', 'N05CF02', 'N06AA04', 'R06AE07', 'G03AA09', 'N02CC03', 'G03AA07', 'N03AE01', 'G03AC09', \n",
    "        'A10BA02', 'H01BA02', 'B03AA07','N06AB04', 'A02BC05', 'N06AX11', 'N05BB01', 'N06AB05', 'N05CD02', 'N05BA04', 'J01FA10', 'D07AC13', 'N01BB20', 'A06AD65', 'A03FA01', 'N06BA02', 'D10AD03', 'G03AA12', 'G03AA13', 'D07BC01', 'S01GX02', 'A02BA02', \n",
    "        'G03AC06', 'NO6BAO9', 'N02CC01', 'N02AA59', 'J01CF01', 'D07AA02', 'R01AD09', 'D07AC01', 'N03AX14', 'G03FB05', 'N03AF01', 'A12BA02', 'D07AB08', 'A06AD11', 'G03AD02', 'S01AA01', 'N02BA01', 'D01AC20', 'R01AC02', 'M01AE01', 'S01GX09', 'A02BC01', \n",
    "        'A12BA01', 'C02AC02', 'D07XC01', 'H02AB02', 'N02BE01', 'R05DA01', 'D06AX01', 'S01AA13', 'J05AB01', 'C09AA02', 'A07AA02', 'D06AX05', 'D10AD53', 'D10AD01', 'N07BA03', 'C07AA05', 'G03CA03', 'N06AX12', 'J01CA08', 'N07BA02', 'D06AA03', 'D07AB02', \n",
    "        'R03BA05', 'G03HB01', 'B03BA03', 'D06BB03', 'R06AX27', 'N01BB02', 'R03BA02', 'R03AC03', 'J01CE02', 'D09AA02', 'N06AA09']]\n",
    "        \n",
    "        dependent_variables = [self.dependent_variable_TNE_NO_180]\n",
    "        dependent_variable_names = {'self.dependent_variable_TNE_NO_180':'TNE_NO_180'}\n",
    "        #for dependent_variable in dependent_variables:\n",
    "        for variable_name, dependent_variable in zip(dependent_variable_names.values(), dependent_variables):\n",
    "            linear_prediction_model = LinearRegression()\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(self.independent_variable, dependent_variable, train_size=0.7)\n",
    "            X_test,X_eval,y_test,y_eval = train_test_split(X_temp,y_temp,test_size=0.33)\n",
    "            linear_prediction_model.fit(X_train,y_train)\n",
    "            y_pred = linear_prediction_model.predict(X_test)\n",
    "            print(f'\\n****** Prediction Model Evaluation result  {variable_name} as dependent variable ******')\n",
    "            y_pred = linear_prediction_model.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            print('R-squared:', r2)\n",
    "            print('Mean squared error:', mse)\n",
    "                \n",
    "\n",
    "SplitMedicationDiagnosisInUniquePieces_Obj = SplitMedicationDiagnosisInUniquePieces(os.getenv(\"FILE_Full_ICD10_ATC_PATH\"))\n",
    "SplitMedicationDiagnosisInUniquePieces_Obj.split_diagnosis_medication_in_unique_pieces()\n",
    "\n",
    "ClassifyReadmissionWithMedicationDiagnosis_Obj = ClassifyReadmissionWithMedicationDiagnosis('Split_Full_ICD10_ATC.csv')\n",
    "merged_df, dependent_variable_TNE_BO_180, independent_variable  = ClassifyReadmissionWithMedicationDiagnosis_Obj.load_data()\n",
    "ClassifyReadmissionWithMedicationDiagnosis_Obj.train_classifier_with_medication_diagnosis()\n",
    "ClassifyReadmissionWithMedicationDiagnosis_Obj.prediction_with_medication_diagnosis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-P2]",
   "language": "python",
   "name": "conda-env-.conda-P2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
